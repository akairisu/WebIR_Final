{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f21d13a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "train_set = sparse.load_npz('train_matrix.npz')\n",
    "test_set = sparse.load_npz('test_matrix.npz')\n",
    "\n",
    "user_mean = []\n",
    "ISBN_mean = []\n",
    "\n",
    "train_data = np.asarray(train_set.tocoo().data, dtype = np.float64)\n",
    "train_rows = train_set.tocoo().row  #user\n",
    "train_cols = train_set.tocoo().col  #book\n",
    "\n",
    "test_data = np.asarray(test_set.tocoo().data, dtype = np.float64)\n",
    "test_rows = test_set.tocoo().row  #user\n",
    "test_cols = test_set.tocoo().col  #book\n",
    "\n",
    "train_mean = np.mean(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09b326de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start user mean\n",
      "end user mean\n"
     ]
    }
   ],
   "source": [
    "#calculate users' mean rating\n",
    "print(\"start user mean\")\n",
    "for i in range(train_set.shape[0]):\n",
    "    index = np.where(train_rows == i)\n",
    "    if len(train_data[index[0]]) == 0:\n",
    "        user_mean.append(train_mean)\n",
    "    else:\n",
    "        user_mean.append(np.mean(train_data[index[0]]))\n",
    "user_mean = train_set.mean(axis=1)\n",
    "print(\"end user mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f65669a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start book mean\n",
      "end book mean\n"
     ]
    }
   ],
   "source": [
    "#calculate books' mean rating\n",
    "print(\"start book mean\")\n",
    "for i in range(train_set.shape[1]):\n",
    "    index = np.where(train_cols == i)\n",
    "    if len(train_data[index[0]]) == 0:\n",
    "        ISBN_mean.append(train_mean)\n",
    "    else:\n",
    "        ISBN_mean.append(np.mean(train_data[index[0]]))\n",
    "        train_data[index[0]] -= np.mean(train_data[index[0]])\n",
    "train_set = sparse.csr_matrix((train_data, (train_rows, train_cols)), shape=train_set.shape)\n",
    "print(\"end book mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "49e253e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = np.zeros(shape=train_set.shape)\n",
    "for i in range(train_set.shape[0]):\n",
    "    bias[i] += user_mean[i]\n",
    "for j in range(train_set.shape[1]):\n",
    "    bias[:,j] += ISBN_mean[j]\n",
    "bias -= train_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "843d4c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10000)\n",
      "[[ 1.00000000e+00  1.38366720e-01  1.70106397e-01 ...  3.66900853e-03\n",
      "  -1.05558092e-03  1.56544201e-02]\n",
      " [ 1.38366720e-01  1.00000000e+00  7.02224805e-02 ...  5.73536118e-03\n",
      "   1.30377977e-03  4.65403185e-03]\n",
      " [ 1.70106397e-01  7.02224805e-02  1.00000000e+00 ...  9.47986052e-04\n",
      "  -6.49974202e-04 -2.47729880e-03]\n",
      " ...\n",
      " [ 3.66900853e-03  5.73536118e-03  9.47986052e-04 ...  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [-1.05558092e-03  1.30377977e-03 -6.49974202e-04 ...  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00]\n",
      " [ 1.56544201e-02  4.65403185e-03 -2.47729880e-03 ...  0.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_sim = cosine_similarity(train_set.transpose())\n",
    "print(cosine_sim.shape)\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "38b2c44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ISBN_mean = np.array(ISBN_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "42be4112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bfdbfdcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start predict default\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [19:43<00:00,  8.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end predict default\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"start predict default\")\n",
    "predict = np.zeros(shape=train_set.shape)\n",
    "for i in tqdm(range(train_set.shape[1])):\n",
    "    sim_book = (-cosine_sim[i]).argsort()[1:51]\n",
    "    pearson_cor_div = 0\n",
    "    cols = train_set[:, sim_book].toarray()\n",
    "    cosine_sims = cosine_sim[i, sim_book]\n",
    "    predict[:, i] = np.dot(cols + ISBN_mean[sim_book] - bias[:, sim_book], cosine_sims) / np.sum(cosine_sims) + bias[:, i]\n",
    "#     print(predict[:,i])\n",
    "print(\"end predict default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a922fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('pearson_predict.npz', predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cc689a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = np.load('pearson_predict.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87b0484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = sparse.load_npz('rating_matrix.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f21adda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 43851/597648 [00:00<00:01, 438440.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start evaluate default\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 597648/597648 [00:01<00:00, 486174.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  681.5263687451162\n",
      "average error =  3.2591557665193204e-08\n",
      "start map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6536/53424 [04:48<34:27, 22.68it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-238c312c60ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mranking\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moriginal_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0mtop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranking\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mtop_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_intXint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_intXslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_intXarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36m_get_intXslice\u001b[0;34m(self, row, col)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_intXslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_submatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0;31m# TODO: uncomment this once it's faster:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;31m# return self.getrow(row)._minor_slice(col)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m_get_submatrix\u001b[0;34m(self, major, minor, copy)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mj0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m         return self.__class__((data, indices, indptr), shape=shape,\n\u001b[0m\u001b[1;32m    796\u001b[0m                               dtype=self.dtype, copy=False)\n\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Read matrix dimensions given, if any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#evaluate\n",
    "from tqdm import tqdm \n",
    "import math\n",
    "import csv\n",
    "\n",
    "print(\"start evaluate default\")\n",
    "#rmse\n",
    "mse = 0\n",
    "for i in tqdm(range(len(test_data))):\n",
    "    error = (predict[test_rows[i]][test_cols[i]] - test_data[i]) ** 2\n",
    "    mse += error\n",
    "rmse = math.sqrt(mse)\n",
    "print(\"rmse = \", rmse)\n",
    "print(\"average error = \", error / len(test_data))\n",
    "\n",
    "#map\n",
    "print(\"start map\")\n",
    "ground_truth = []\n",
    "for i in range(train_set.shape[0]):\n",
    "    ground_truth.append([])\n",
    "with open('to_read.csv', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for data in reader:\n",
    "        ground_truth[int(data['user_id']) - 1].append(int(data['book_id']) - 1)\n",
    "        \n",
    "mAP = 0\n",
    "count = 0\n",
    "for i in tqdm(range(train_set.shape[0])):\n",
    "    ground_len = len(ground_truth[i])\n",
    "    if ground_len == 0:\n",
    "        continue\n",
    "    else:\n",
    "        ranking = (-predict[i]).argsort()\n",
    "        top_count = 0\n",
    "        top = []\n",
    "        for j in range(len(ranking)):\n",
    "            if ranking[j] not in original_data[i].indices:\n",
    "                top.append(ranking[j])\n",
    "                top_count += 1\n",
    "                if top_count == ground_len * 50:\n",
    "                    break\n",
    "        count = count + 1\n",
    "        ap = 0\n",
    "        correct_count = 0\n",
    "        for j in range(len(top)):\n",
    "            if top[j] in ground_truth[i]:\n",
    "                correct_count = correct_count + 1\n",
    "                ap = ap + correct_count / (j + 1)\n",
    "        if correct_count != 0:\n",
    "            ap = ap / ground_len\n",
    "        else:\n",
    "            ap = 0\n",
    "        mAP = mAP + ap\n",
    "mAP = mAP / count\n",
    "print(\"mAP:\", mAP)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bf92bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e79dabcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "train_set = sparse.load_npz('train_matrix.npz')\n",
    "test_set = sparse.load_npz('test_matrix.npz')\n",
    "\n",
    "user_mean = []\n",
    "ISBN_mean = []\n",
    "\n",
    "train_data = np.asarray(train_set.tocoo().data, dtype = np.float64)\n",
    "train_rows = train_set.tocoo().row  #user\n",
    "train_cols = train_set.tocoo().col  #book\n",
    "\n",
    "test_data = np.asarray(test_set.tocoo().data, dtype = np.float64)\n",
    "test_rows = test_set.tocoo().row  #user\n",
    "test_cols = test_set.tocoo().col  #book\n",
    "\n",
    "train_mean = np.mean(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baa58b25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start user mean\n",
      "[3.6        4.46428571 1.76923077 ... 4.25225225 4.44776119 4.40495868]\n",
      "end user mean\n"
     ]
    }
   ],
   "source": [
    "#calculate users' mean rating\n",
    "print(\"start user mean\")\n",
    "user_mean = np.array(train_set.mean(axis=1)).flatten()\n",
    "nonzeros = np.diff(train_set.indptr)\n",
    "user_mean = user_mean * train_set.shape[1] / nonzeros\n",
    "print(user_mean)\n",
    "print(\"end user mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd9cbe8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start book mean\n",
      "[4.27734242 4.35082451 3.2184571  ... 4.32978723 3.7008547  4.01      ]\n",
      "end book mean\n"
     ]
    }
   ],
   "source": [
    "#calculate books' mean rating\n",
    "print(\"start book mean\")\n",
    "train_set = train_set.tocsc()\n",
    "ISBN_mean = np.array(train_set.mean(axis=0)).flatten()\n",
    "nonzeros = []\n",
    "for i in range(train_set.shape[1]):\n",
    "    nonzeros.append(len(train_set[:, i].data))\n",
    "nonzeros = np.array(nonzeros)\n",
    "ISBN_mean = ISBN_mean * train_set.shape[0] / nonzeros\n",
    "print(ISBN_mean)\n",
    "train_set = train_set.tocsr()\n",
    "print(\"end book mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3399822",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = np.zeros(shape=train_set.shape)\n",
    "for i in range(train_set.shape[0]):\n",
    "    bias[i] += user_mean[i]\n",
    "for j in range(train_set.shape[1]):\n",
    "    bias[:,j] += ISBN_mean[j]\n",
    "bias -= train_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33ebcc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10000)\n",
      "[[1.         0.52834589 0.50534332 ... 0.01548447 0.03015078 0.01044099]\n",
      " [0.52834589 1.         0.46841649 ... 0.0206089  0.0210829  0.01384943]\n",
      " [0.50534332 0.46841649 1.         ... 0.00423889 0.01458847 0.00197905]\n",
      " ...\n",
      " [0.01548447 0.0206089  0.00423889 ... 1.         0.         0.        ]\n",
      " [0.03015078 0.0210829  0.01458847 ... 0.         1.         0.        ]\n",
      " [0.01044099 0.01384943 0.00197905 ... 0.         0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_sim = cosine_similarity(train_set.transpose())\n",
    "print(cosine_sim.shape)\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d63f481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, mat, sim, bias):\n",
    "        self.mat = mat.toarray()\n",
    "        self.data = mat.tocoo().data\n",
    "        self.rows = mat.tocoo().row\n",
    "        self.cols = mat.tocoo().col\n",
    "        self.simbook = []\n",
    "        for i in tqdm(range(mat.shape[1])):\n",
    "            self.simbook.append((-sim[i]).argsort()[1:21])\n",
    "        self.bias = bias\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.cols[index], self.simbook[self.cols[index]], self.mat[self.rows[index], self.simbook[self.cols[index]]], \\\n",
    "            self.bias[self.rows[index]][self.cols[index]], self.bias[self.rows[index]][self.simbook[self.cols[index]]]\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c45b3b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IW(nn.Module):\n",
    "    def __init__(self, book_num):\n",
    "        super(IW, self).__init__()\n",
    "        self.w = nn.Parameter(torch.empty(book_num, book_num))\n",
    "        \n",
    "#         nn.init.orthogonal_(self.w.weight)\n",
    "    \n",
    "    def forward(self, i, sim_book, bias_i, bias_j, predict, ground_truth, train=True):\n",
    "        predict = predict - bias_j\n",
    "        product = torch.zeros(sim_book.shape[0], dtype=torch.float32, device=device)\n",
    "        predict = predict.to(torch.float32)\n",
    "        for j in range(len(sim_book)):\n",
    "            product[j] = torch.dot(self.w[i[j], sim_book[j]], predict[j])\n",
    "        return ((bias_i + product - ground_truth) ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb9afb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:05<00:00, 1763.08it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1768.10it/s]\n"
     ]
    }
   ],
   "source": [
    "model_train_set = dataset(sparse.load_npz('train_matrix.npz'), cosine_sim, bias)\n",
    "model_test_set = dataset(sparse.load_npz('test_matrix.npz'), cosine_sim, bias)\n",
    "\n",
    "train_loader = DataLoader(model_train_set, batch_size = 128, shuffle=True)\n",
    "test_loader = DataLoader(model_test_set, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "18cfa575",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/42023 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 12323/42023 [1:53:59<4:36:38,  1.79it/s, loss=108.900, lr=0.0010 ]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 41%|████      | 17280/42023 [2:39:56<3:48:16,  1.81it/s, loss=107.428, lr=0.0010 ]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 53%|█████▎    | 22319/42023 [3:26:28<3:01:54,  1.81it/s, loss=128.232, lr=0.0010 ]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 74%|███████▍  | 31033/42023 [4:46:58<1:41:29,  1.80it/s, loss=121.882, lr=0.0010 ]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 42023/42023 [6:28:35<00:00,  1.80it/s, loss=13.294 , lr=0.0010 ]  \n",
      "  0%|          | 10/4670 [00:00<00:49, 94.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 training loss: 111.918788012993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4670/4670 [00:46<00:00, 99.52it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 validation loss: tensor(138.4365, device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 519/42023 [04:49<6:25:58,  1.79it/s, loss=97.973 , lr=0.0010 ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-aaa744a5cc4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mbxj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbxj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimbook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbxj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrxj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mloss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "model = IW(train_set.shape[1]).to(device)\n",
    "model.load_state_dict(torch.load('IW_model.pkl', map_location=device))\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=2)\n",
    "epoch_num = 50\n",
    "\n",
    "min_loss = 1000\n",
    "\n",
    "print(\"start training\")\n",
    "for epoch in range(epoch_num):\n",
    "    model.train()\n",
    "    loss_sum = 0.0\n",
    "    iterator = tqdm(train_loader)\n",
    "    for rxi, i, simbook, rxj, bxi, bxj in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        rxi = rxi.to(device)\n",
    "        i = i.to(device)\n",
    "        simbook = simbook.to(device)\n",
    "        rxj = rxj.to(device).float()\n",
    "        bxi = bxi.to(device)\n",
    "        bxj = bxj.to(device).float()\n",
    "        loss = model(i.long(), simbook, bxi, bxj, rxj, rxi)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "        iterator.set_postfix_str('loss={:^7.3f}, lr={:^7.4f}'.format(loss, optimizer.param_groups[0][\"lr\"]))\n",
    "    \n",
    "    model.eval()\n",
    "    loss_avg = loss_sum / len(train_loader)\n",
    "    print(\"Epoch:\", epoch, \"training loss:\", loss_avg)\n",
    "    \n",
    "    valid_loss = 0\n",
    "    iterator = tqdm(test_loader)\n",
    "    for rxi, i, simbook, rxj, bxi, bxj in iterator:\n",
    "        rxi = rxi.to(device)\n",
    "        i = i.to(device)\n",
    "        simbook = simbook.to(device)\n",
    "        rxj = rxj.to(device)\n",
    "        bxi = bxi.to(device)\n",
    "        bxj = bxj.to(device)\n",
    "        with torch.no_grad():\n",
    "            valid_loss += model(i.long(), simbook, bxi, bxj, rxj, rxi)\n",
    "    \n",
    "    valid_avg = valid_loss / len(test_loader)\n",
    "    print(\"Epoch:\", epoch, \"validation loss:\", valid_avg)\n",
    "    \n",
    "    if valid_avg < min_loss:\n",
    "        min_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'IW_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25e82d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'IW_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ccf8a7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4670/4670 [00:46<00:00, 99.48it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  804.0513647410631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "model = IW(train_set.shape[1]).to(device)\n",
    "model.load_state_dict(torch.load('IW_model.pkl', map_location=device))\n",
    "model.eval()\n",
    "valid_loss = 0\n",
    "iterator = tqdm(test_loader)\n",
    "for rxi, i, simbook, rxj, bxi, bxj in iterator:\n",
    "    rxi = rxi.to(device)\n",
    "    i = i.to(device)\n",
    "    simbook = simbook.to(device)\n",
    "    rxj = rxj.to(device)\n",
    "    bxi = bxi.to(device)\n",
    "    bxj = bxj.to(device)\n",
    "    with torch.no_grad():\n",
    "        valid_loss += model(i.long(), simbook, bxi, bxj, rxj, rxi)\n",
    "rmse = math.sqrt(valid_loss)\n",
    "print(\"rmse = \", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eb7d5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 53424)\n",
      "[[[5.0364695  4.9267977  4.02035965 ... 4.32978723 3.7008547  4.01      ]\n",
      "  [4.94111829 4.90082773 3.94585828 ... 4.32978723 3.7008547  4.01      ]\n",
      "  [4.92959934 4.92429346 3.9469804  ... 4.32978723 3.7008547  4.01      ]\n",
      "  ...\n",
      "  [4.16962864 4.25740855 3.16808728 ... 4.32978723 3.7008547  4.01      ]\n",
      "  [3.37458481 2.9547447  2.3961622  ... 4.32978723 3.71854408 4.01      ]\n",
      "  [3.70905273 3.81092423 2.6129528  ... 4.32978723 3.7008547  4.01      ]]]\n",
      "[[5.0364695  4.9267977  4.02035965 ... 4.32978723 3.7008547  4.01      ]\n",
      " [4.94111829 4.90082773 3.94585828 ... 4.32978723 3.7008547  4.01      ]\n",
      " [4.92959934 4.92429346 3.9469804  ... 4.32978723 3.7008547  4.01      ]\n",
      " ...\n",
      " [4.16962864 4.25740855 3.16808728 ... 4.32978723 3.7008547  4.01      ]\n",
      " [3.37458481 2.9547447  2.3961622  ... 4.32978723 3.71854408 4.01      ]\n",
      " [3.70905273 3.81092423 2.6129528  ... 4.32978723 3.7008547  4.01      ]]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from scipy import sparse\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "personal_rating = np.zeros((1, predict.shape[1]))\n",
    "book_list = [1, 2, 4, 5, 1024, 2048, 4096, 8192]\n",
    "for books in book_list:\n",
    "    personal_rating[0][books] = 5\n",
    "cosine_sim = cosine_similarity(personal_rating, predict)\n",
    "print(cosine_sim.shape)\n",
    "top_sim = (-cosine_sim).argsort()[:20]\n",
    "top_rating = [predict[i] for i in top_sim]\n",
    "print(top_rating)\n",
    "top_mean = top_rating.mean(axis=0)\n",
    "print(top_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "633c6976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04400208 0.09757142 0.05702144 ... 0.06503325 0.12511975 0.09648062]]\n",
      "Recommended:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'id_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-0f9eed15b275>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Recommended:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbooks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecommendation_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooks\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbooks\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'id_dict' is not defined"
     ]
    }
   ],
   "source": [
    "personal_rating = np.zeros((1, predict.shape[1]))\n",
    "for books in book_list:\n",
    "    personal_rating[0][books - 1] = 5\n",
    "cos = cosine_similarity(personal_rating, original_data)\n",
    "print(cos)\n",
    "top_sim = (-cos).argsort()[:20]\n",
    "top_rating = predict[top_sim].squeeze(0)\n",
    "top_mean = top_rating.mean(axis=0)\n",
    "recommendation_list = (-top_mean).argsort()[:20]\n",
    "print(\"Recommended:\")\n",
    "for books in recommendation_list:\n",
    "    print(books + 1, id_dict[books + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75161f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.0364695  4.9267977  4.02035965 ... 4.32978723 3.7008547  4.01      ]\n",
      " [4.94111829 4.90082773 3.94585828 ... 4.32978723 3.7008547  4.01      ]\n",
      " [4.92959934 4.92429346 3.9469804  ... 4.32978723 3.7008547  4.01      ]\n",
      " ...\n",
      " [4.16962864 4.25740855 3.16808728 ... 4.32978723 3.7008547  4.01      ]\n",
      " [3.37458481 2.9547447  2.3961622  ... 4.32978723 3.71854408 4.01      ]\n",
      " [3.70905273 3.81092423 2.6129528  ... 4.32978723 3.7008547  4.01      ]]\n"
     ]
    }
   ],
   "source": [
    "print(top_rating.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477fbc45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
